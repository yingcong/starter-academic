---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Learning to Know Where to See: A Visibility-Aware Approach for Occluded Person Re-identification'
subtitle: ''
summary: ''
authors:
- Jinrui Yang
- Jiawei Zhang
- Fufu Yu
- Xinyang Zhang
- Mengdan Zhang
- Xin Sun
- admin
- Wei-Shi Zheng
tags: ['Person Re-identification']
categories: []
date: '2021-07-01'
# lastmod: 2021-06-12T11:07:48+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-06-12T03:07:48.457169Z'
publication_types:
- '1'
abstract: '
Person re-identification (ReID) has gained an impressiveprogress in recent years. However, the occlusion is still acommon and challenging problem for recent ReID methods.Several mainstream methods utilize extra cues (e.g., humanpose information) to distinguish human parts from obstacles to alleviate the occlusion problem. Although achievinginspiring progress, these methods severely rely on the fine-grained extra cues, and are sensitive to the estimation errorin the extra cues. In this paper, we show that existing methods may degrade if the extra information is sparse or noisy.Thus we propose a simple yet effective method that is robustto sparse and noisy pose information. This is achieved bydiscretizing pose information to the visibility label of bodyparts, so as to suppress the influence of occluded regions.We show in our experiments that leveraging pose informa-tion in this way is more effective and robust. Besides, ourmethod can be embedded into most person ReID modelseasily. Extensive experiments show that our method outperforms the state-of-the-art. We will release the source codesafter the paper is accepted.
'
publication: 'In *Proceedings of the IEEE International Conference on Computer Vision*'
# publication_short: 'In *CVPR*'
# url_pdf: 'https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.html'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
---
